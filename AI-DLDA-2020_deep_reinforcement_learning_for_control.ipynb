{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aidlda_deep_reinforcement_learning_for_control.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxyxoXxNykx8",
        "colab_type": "text"
      },
      "source": [
        "#Deep Reinforcement Learning for Control\n",
        "\n",
        "## Lab Session\n",
        "\n",
        "### 3rd International Summer School on Artificial Intelligience AI-DLDA 2020\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Notebook written by Matteo Dunnhofer `matteo.dunnhofer@uniud.it`\n",
        "\n",
        "Machine Learning and Perception Lab\n",
        "\n",
        "University of Udine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toPmU8ezywR2",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we see how to solve a toy control problem with **reinforcement learning** (RL) techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1gty_LkgM1N",
        "colab_type": "text"
      },
      "source": [
        "## Problem\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/cpow-89/Extended-Deep-Q-Learning-For-Open-AI-Gym-Environments/master/images/Lunar_Lander_v2.gif)\n",
        "\n",
        "The **goal** is to control a lunar module to land on the moon, without having access to any ground-truth or prior information. \n",
        "\n",
        "To solve this problem, we will train an **artificial agent** to by trial-and-error. The agent will control the module and learn by the experience acquired through the **interaction** between the module and the lunar environment. The interaction will happen through the **observation** of some features of the lander module, the subsequent execution of **actions**, and the **rewarding** of the latter based on their quality. The agent will be implemented as a **neural network** and the state-of-the-art **Soft Actor-Critic** (SAC) algorithm will be employed to optimize the network weights. \n",
        "\n",
        "We will use the most popular tools in the RL landscape to implement and solve this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSVDIJcGirJN",
        "colab_type": "text"
      },
      "source": [
        "## Utilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIi2wSMYerEm",
        "colab_type": "text"
      },
      "source": [
        "Let's start by fixing the seed for the random number generators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrKER7tnPB47",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fix the seed for random number generators\n",
        "SEED = 123"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNepi_lt9nsF",
        "colab_type": "text"
      },
      "source": [
        "The following cells implement utilities to visualize graphically (as a video) the interaction between the agent and the environment. You do not need this part if you run the notebook as a script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTfyw8373oEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7BZ_YCU3ojF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsyHMI673o-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kTcD1TzFDM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "def show_video():\n",
        "    mp4list = glob.glob('videos/*/*.mp4')\n",
        "    mp4list.sort(key=os.path.getmtime)\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[-1]\n",
        "        video = io.open(mp4, 'rb').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "    env = Monitor(env, './videos/' + str(time.time()) + '/')  # Monitor objects are used to save interactions as videos\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBCVgMue0VcP",
        "colab_type": "text"
      },
      "source": [
        "## Environment\n",
        "\n",
        "Let's define the environment that the agent will interact with to pursue its goals. We will take advantage of OpenAi's [``gym``](https://gym.openai.com) which provides a large number of predefined environments for RL experiments.\n",
        "\n",
        "In particular, we will use the ``LunarLanderContinuous-v2`` environment. This env provides 8 features as **states** (position of the lander, velocity, angles, terrain contact sensors) and requires a continuous action space of 2 **actions** (amount of fire for left/right engine, amount of fire for main engine). **Rewards** are given based on the quality of the descent and the landing position, and on the usage of the fuel. Interactions end if the lander crashes or when it lands succesfully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmB3a1pGqEAs",
        "colab_type": "text"
      },
      "source": [
        "Let's install `gym`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0g8fVIASu1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install packages related to gym and gym, necessary for LunarLanderContinuous-v2\n",
        "!pip install box2d-py > /dev/null 2>&1\n",
        "!pip install gym > /dev/null 2>&1\n",
        "!pip install gym[Box_2D] > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVVGZo-UqIsh",
        "colab_type": "text"
      },
      "source": [
        "And now let's instantiate the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc1tOYD-Aywj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment 'LunarLanderContinuous-v2'\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "env.seed(SEED)\n",
        "\n",
        "# Print the observation and action spaces of the env\n",
        "print('State space: {}'.format(env.observation_space))\n",
        "print('Action space: {} - low: {} high: {}'.format(env.action_space, env.action_space.low[0], env.action_space.high[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceNu6T6Xl6o4",
        "colab_type": "text"
      },
      "source": [
        "We can look into the environment definition [here](https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDm2APXyCZs4",
        "colab_type": "text"
      },
      "source": [
        "## Agent\n",
        "\n",
        "Let's move now to the agent definition! The agent will be the controller of the lander, it will contain the policy, and it will be responsible for its learning and execution.\n",
        "\n",
        "Again, we will take advantage of an OpenAI's library, this time [Spinning Up in Deep RL!](https://spinningup.openai.com/en/latest/)\n",
        "\n",
        "It's an excellent resource to start working with RL, and\n",
        "it includes basic and advanced RL algorithms implementations, with very detailed explanations. \n",
        "Algorithms and models are implemented by means of both PyTorch and TensorFlow frameworks. In our case, we will use the PyTorch version.\n",
        "\n",
        "Let's install it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVbFgODEnKPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/openai/spinningup.git > /dev/null 2>&1\n",
        "!pip install -e spinningup > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIEGyhtbUez3",
        "colab_type": "text"
      },
      "source": [
        "Let's see how to exploit the library to define our agent.\n",
        "As we are going to use the SAC algorithm, we need to implement an object that fits the `actor_critic` parameter required by the [`spinup.sac_pytorch()`](https://spinningup.openai.com/en/latest/algorithms/sac.html#documentation-pytorch-version) function (which implements the SAC learning).\n",
        "\n",
        "We can do this by defining a custom `torch.nn.Module` that respects such requirements, or just use the `MLPActorCritic` implementation given by OpenAI.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "719OxpnImeRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spinup.algos.pytorch.sac.core import MLPActorCritic\n",
        "\n",
        "# Instantiate the agent as an actor-critic agent composed of multi-layer perceptrons\n",
        "agent = MLPActorCritic(env.observation_space, env.action_space)\n",
        "\n",
        "print(agent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDyBpexJY2Mf",
        "colab_type": "text"
      },
      "source": [
        "This is just a `torch.nn.Module` that contains an MLP for the stochastic policy `pi` (the actor), and two MLPs for the Q-value functions `q1` and `q2` (the critics). We can have a look to the detailed implementation [here](https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/sac/core.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-pB1h1c55Yh",
        "colab_type": "text"
      },
      "source": [
        "## Interaction\n",
        "\n",
        "Let's move to the implementatioon of the finite-horizon interaction procedure that must happen between agent and environment.\n",
        "This is done by means of the ``run_episode`` function. Following the [general structure of `gym`'s environments](https://gym.openai.com/docs/#observations), a state will be obtained from the environment and it will be given in input to the agent which will produce its action. Then, the action will be passed to the environment which will retirn the reward. This procedure will run until the stop conditions are met.\n",
        "\n",
        "We will use this function for quantitive and qualitative evaluations, but a similar procedure is implemented by the SAC algorithm during training. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r-Ass_gNcrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "def run_episode(agent, env, render=False):\n",
        "    \"\"\"\n",
        "    Given agent and env, runs an episode and returns the obtained rewards\n",
        "\n",
        "    Args:\n",
        "        agent: an object respecting the spinup.sac_pytorch actor_critic parameter\n",
        "        env: a gym environment\n",
        "\n",
        "    Returns:\n",
        "        rewards: list of scalar rewards\n",
        "    \"\"\"\n",
        "    # Empty lists to save rewards\n",
        "    rewards = []        # for rewards\n",
        "\n",
        "    if render:\n",
        "        # Do not need this if you locally run this notebook as a script\n",
        "        env = wrap_env(env)\n",
        "\n",
        "    # Reset environment to the first state\n",
        "    state = env.reset() \n",
        "    done = False            # signal from environment that episode is over\n",
        "\n",
        "    # Run until the episode is finished\n",
        "    while not done:\n",
        "\n",
        "        # Render environment to screen\n",
        "        if render:\n",
        "            env.render()\n",
        "\n",
        "        # Get the action from the agent for the current state\n",
        "        action = agent.act(torch.tensor(state, dtype=torch.float32))\n",
        "        \n",
        "        # Perform action and receive reward and new state\n",
        "        state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Save reward\n",
        "        rewards.append(reward)\n",
        "\n",
        "    if render:\n",
        "        # Do not need this if you locally run this notebook as a script\n",
        "        env.close()\n",
        "        show_video()\n",
        "\n",
        "    return rewards\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xSot9LXXRg4",
        "colab_type": "text"
      },
      "source": [
        "Let's use `run_episode` to see how the agent performs without training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEEszek5XR1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rewards = run_episode(agent, env, render=True)\n",
        "\n",
        "print('R(tau) = {}'.format(sum(rewards)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MUAGDSX-KL2",
        "colab_type": "text"
      },
      "source": [
        "## Training\n",
        "\n",
        "Let's arrive to the actual learning phase!\n",
        "\n",
        "As we said, we will use the Soft Actor-Critic algorithm, which is a state-of-the-art off-policy RL algorithm particularly designed for robotics and control problems. \n",
        "\n",
        "Here are some features:\n",
        "\n",
        "\n",
        "*   Stochastic Policy optimization in an Off-Policy way\n",
        "    - It bridges the sample efficiency of off-policy methods with the stability of policy optimization\n",
        "\n",
        "*   Entropy Regularization\n",
        "    - Maximization of policy entropy for better exploration\n",
        "\n",
        "\n",
        "*   Double Critic trick\n",
        "    - To reduce bias and make learning faster\n",
        "\n",
        "\n",
        "\n",
        "For the details of the algorithm, Spinning Up in Deep RL! gives a [very good tutorial](https://spinningup.openai.com/en/latest/algorithms/sac.html). Here we can have a look to the pseudocode.\n",
        "\n",
        "![alt text](https://imgur.com/oNsh1a8.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyYed8bDAmFD",
        "colab_type": "text"
      },
      "source": [
        "With the `spinup` implementation is just matter of a single statement. If you are interested how it is defined inside, check [here](https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/sac/sac.py).\n",
        "\n",
        "Now let's train!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXEObBTW3VGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spinup import sac_pytorch\n",
        "\n",
        "sac_pytorch(lambda: gym.make('LunarLanderContinuous-v2'),\n",
        "            actor_critic=MLPActorCritic,\n",
        "            seed=SEED,\n",
        "            epochs=50,\n",
        "            logger_kwargs={'output_dir' : './experiments'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_i955PXhoaA_",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "Let's see how our trained agent performs!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKnlIsm_t-j4",
        "colab_type": "text"
      },
      "source": [
        "Download the pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTg3FlRtscUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O agent-trained.pt https://github.com/dontfollowmeimcrazy/AI-DLDA-2020-deep-reinforcement-learning-for-control/blob/master/agent-trained.pt?raw=true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vEJ9dB2uEcs",
        "colab_type": "text"
      },
      "source": [
        "And then run the agent!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLSfnz-5oaci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = torch.load('agent-trained.pt')\n",
        "\n",
        "# Qualitatively evaluate the performance of the agent on 5 episodes\n",
        "for e in range(5):\n",
        "    rewards = run_episode(agent, env, render=True)\n",
        "\n",
        "    R = sum(rewards)\n",
        "\n",
        "    print('Test episode {} - R(tau) = {}'.format(e, R))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN8w0afbNpST",
        "colab_type": "text"
      },
      "source": [
        "Not bad!"
      ]
    }
  ]
}